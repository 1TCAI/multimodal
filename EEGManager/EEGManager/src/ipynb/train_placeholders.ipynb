{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from six.moves import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "from tensorflow.python.client import timeline\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"import done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pickled dataset into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "name_pickle = '../data/train/trainsh1.pickle'\n",
    "\n",
    "with open(name_pickle, 'rb') as f:\n",
    "    print('Unpickling ' + name_pickle)\n",
    "    load = pickle.load(f)\n",
    "    dataset = load['data']\n",
    "    labels = load['labels']\n",
    "    del load\n",
    "    print('dataset shape:', dataset.shape)\n",
    "    print('labels shape:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reformat data for training\n",
    "- Divide each file with 240000 samples into smaller batch_samples ~= size of receptive field of eegnet\n",
    "- Keep valid_dataset nr of samples intact for proper validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_array(array):\n",
    "    # Normalize mean=0 and sigma=0.25: axis=0 is along columns, vertical lines.\n",
    "    array -= np.mean(array, axis=0) \n",
    "    array /= 2*np.ptp(array, axis=0)\n",
    "    return array\n",
    "    \n",
    "def clean_normalize_data_labels(data, labels, sigma=0.5):\n",
    "    data_tmp = list()\n",
    "    labels_tmp = list()\n",
    "    for idx, d in enumerate(data):\n",
    "        if (np.count_nonzero(d) < 10) or (np.any(np.std(d, axis=0) < sigma)):\n",
    "            continue\n",
    "        d = normalize_array(d)\n",
    "        data_tmp.append(d)\n",
    "        labels_tmp.append(labels[idx])\n",
    "    return np.asarray(data_tmp), np.asarray(labels_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Output size of the layer\n",
    "num_labels = 2\n",
    "\n",
    "#60% for train and 40% for validation\n",
    "split_idx = int(dataset.shape[0]*0.8)\n",
    "#nr of splits\n",
    "nrOfSplits = 100\n",
    "\n",
    "def format_data(data, labels, nr_splits):\n",
    "    shape = data.shape\n",
    "    # reshape [batch, samples, channels] into [batch * samples, channels]\n",
    "    data = np.reshape(data, (shape[0]*shape[1], shape[2]))\n",
    "    # Split 2D array into the desired smaller chuncks\n",
    "    data = np.asarray(np.split(data, shape[0]*nr_splits, axis=0))\n",
    "    # labels are obtained by repeating original labels nr_splits times\n",
    "    labels = np.repeat((np.arange(num_labels) == labels[:,None]).astype(np.float32), nr_splits, axis=0)\n",
    "    # normalize and eliminate batches that only contain drop-outs\n",
    "    data, labels = clean_normalize_data_labels(data, labels, 0.01)\n",
    "    # data has to be 4D for tensorflow (insert an empty dimension)\n",
    "    data = data[:,None,:,:]\n",
    "    # shuffle data and labels mantaining relation between them. Important after the small batches.\n",
    "    shuffle_idx = np.random.permutation(data.shape[0])\n",
    "    data = data[shuffle_idx,:,:,:]\n",
    "    labels = labels[shuffle_idx]\n",
    "    return data, labels\n",
    "\n",
    "# shuffle file data\n",
    "shuffle_idx = np.random.permutation(dataset.shape[0])\n",
    "dataset = dataset[shuffle_idx,:,:]\n",
    "labels = labels[shuffle_idx]\n",
    "# format and split data into smaller chunks\n",
    "train_dataset, train_labels = format_data(dataset[:split_idx], labels[:split_idx], nrOfSplits)\n",
    "valid_dataset, valid_labels = format_data(dataset[split_idx:-1], labels[split_idx:-1], nrOfSplits)\n",
    "del dataset, labels\n",
    "\n",
    "valid_dataset = valid_dataset[:200]\n",
    "valid_labels = valid_labels[:200]\n",
    "\n",
    "print('train_dataset shape:', train_dataset.shape, 'train_labels shape:', train_labels.shape, \n",
    "      'mix:', float(np.count_nonzero(train_labels[:,1], axis=0))/train_labels.shape[0])\n",
    "print('valid_dataset shape:', valid_dataset.shape, 'valid_labels shape:', valid_labels.shape, \n",
    "      'mix:', float(np.count_nonzero(valid_labels[:,1], axis=0))/valid_labels.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot some data to have an idea of how data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_dataset[1,0,:,0])\n",
    "plt.plot(train_dataset[3,0,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EEGNET implementation\n",
    "\n",
    "Part of https://arxiv.org/pdf/1609.03499.pdf that most concerns classification:\n",
    "\"As a last experiment we looked at speech recognition with WaveNets on the TIMIT (Garofolo et al., 1993) dataset. For this task we added a mean-pooling layer after the dilation convolutions that aggregated the activations to coarser frames spanning 10 milliseconds (160 x downsampling). The pooling layer was followed by a few non-causal convolutions. We trained WaveNet with two loss terms, one to predict the next sample and one to classify the frame, the model generalized better than with a single loss and achieved 18.8 PER on the test set, which is to our knowledge the best score obtained from a model trained directly on raw audio on TIMIT.\"\n",
    "\n",
    "Look into: http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43290.pdf\n",
    "\"Input: This layer extracts 275 ms waveform segments from each of M input microphones. Successive inputs are hopped by 10ms. At the 16kHz sampling rate used in our experiments each segment contains M X 4401 dimensions.\"\n",
    "...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#How many files are supplied per batch.\n",
    "batch_size=16\n",
    "#Number of samples in each batch entry\n",
    "batch_samples=train_dataset.shape[2]\n",
    "#How many filters to learn for the input.\n",
    "input_channels=16\n",
    "#How many filters to learn for the residual.\n",
    "residual_channels=2*input_channels\n",
    "# size after pooling layer\n",
    "pool_size = 2400\n",
    "# convolution filters width\n",
    "filter_width=3\n",
    "\n",
    "def network(batch_data, reuse=False, is_training=True):\n",
    "    with tf.variable_scope('eegnet_network', reuse=reuse):\n",
    "        with slim.arg_scope([slim.batch_norm], \n",
    "                            is_training=is_training):\n",
    "            with slim.arg_scope([slim.conv2d, slim.fully_connected], \n",
    "                                weights_initializer=slim.xavier_initializer(), \n",
    "                                normalizer_fn=slim.batch_norm):\n",
    "                with tf.variable_scope('input_layer'):\n",
    "                    hidden = slim.conv2d(batch_data, residual_channels, [1, filter_width], stride=1, rate=1, \n",
    "                                         activation_fn=None, scope='conv1')\n",
    "\n",
    "                with tf.variable_scope('hidden'):\n",
    "                    with tf.variable_scope('layer1'):\n",
    "                        layer_input = hidden\n",
    "                        hidden = slim.conv2d(hidden, 2*residual_channels, [1, filter_width], stride=1, rate=2, \n",
    "                                             activation_fn=None, scope='dilconv')\n",
    "                        filtr, gate = tf.split(3, 2, hidden) # split features in half\n",
    "                        hidden = tf.mul(tf.tanh(filtr), tf.sigmoid(gate), name='filterXgate')\n",
    "                        hidden = slim.conv2d(hidden, residual_channels, 1, activation_fn=None, scope='1x1skip')\n",
    "                        skip = hidden # skip conn\n",
    "                        hidden = tf.add(hidden, layer_input) # residual conn\n",
    "                    with tf.variable_scope('layer2'):\n",
    "                        layer_input = hidden\n",
    "                        hidden = slim.conv2d(hidden, 2*residual_channels, [1, filter_width], stride=1, rate=4, \n",
    "                                             activation_fn=None, scope='dilconv')\n",
    "                        filtr, gate = tf.split(3, 2, hidden) # split features in half\n",
    "                        hidden = tf.mul(tf.tanh(filtr), tf.sigmoid(gate), name='filterXgate')\n",
    "                        hidden = slim.conv2d(hidden, residual_channels, 1, activation_fn=None, scope='1x1skip')\n",
    "                        skip = tf.add(skip, hidden) # skip conn\n",
    "                        hidden = tf.add(hidden, layer_input) # residual conn\n",
    "                    with tf.variable_scope('layer3'):\n",
    "                        hidden = slim.conv2d(hidden, 2*residual_channels, [1, filter_width], stride=1, rate=8, \n",
    "                                             activation_fn=None, scope='dilconv')\n",
    "                        filtr, gate = tf.split(3, 2, hidden) # split features in half\n",
    "                        hidden = tf.mul(tf.tanh(filtr), tf.sigmoid(gate), name='filterXgate')\n",
    "                        hidden = slim.conv2d(hidden, residual_channels, 1, activation_fn=None, scope='1x1skip')\n",
    "                        skip = tf.add(skip, hidden) # skip conn\n",
    "\n",
    "                with tf.variable_scope('skip_processing'):\n",
    "                    hidden = tf.nn.relu(skip)\n",
    "                    hidden = slim.avg_pool2d(hidden, [1, batch_samples*2//pool_size], [1, batch_samples//pool_size])\n",
    "                    # 1 x 2400 x residual_channels\n",
    "                    hidden = slim.conv2d(hidden, 32, 1, activation_fn=tf.nn.relu, scope='1x1compress1')\n",
    "                    hidden = slim.conv2d(hidden, 16, [1, 8], stride=4, activation_fn=tf.nn.relu, scope='1x5reduce1')\n",
    "                    # 1 x 600 x 16\n",
    "                    hidden = slim.conv2d(hidden, 8, 1, activation_fn=tf.nn.relu, scope='1x1compress2')\n",
    "                    hidden = slim.conv2d(hidden, 4, [1, 8], stride=4, activation_fn=tf.nn.relu, scope='1x5reduce2')\n",
    "                    # 1 x 150 x 4\n",
    "                    hidden = slim.conv2d(hidden, 2, 1, activation_fn=tf.nn.relu, scope='1x1compress3')\n",
    "                    hidden = slim.conv2d(hidden, 2, [1, 6], stride=3, activation_fn=tf.nn.relu, scope='1x5reduce3')\n",
    "                    # 1 x 75 x 2\n",
    "\n",
    "                with tf.variable_scope('logits'):\n",
    "                    hidden = slim.dropout(hidden, 0.7, is_training=is_training)\n",
    "                    hidden = slim.flatten(hidden)\n",
    "                    logits = slim.fully_connected(hidden, num_labels, activation_fn=None, \n",
    "                                                  normalizer_fn=None, scope='fc1')\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#number of steps after which learning rate is decayed\n",
    "decay_steps=500\n",
    "\n",
    "\n",
    "#Construct computation graph\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 1, batch_samples, input_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.int32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset, dtype=tf.float32)\n",
    "    tf_valid_labels = tf.constant(valid_labels, dtype=tf.int32)\n",
    "\n",
    "    with tf.name_scope('eegnet_handling'):\n",
    "        logits = network(tf_train_dataset)\n",
    "        loss = slim.losses.softmax_cross_entropy(logits, tf_train_labels, scope='loss')\n",
    "        tf.scalar_summary('loss', loss)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=1e-3, epsilon=1e-4).minimize(loss, \n",
    "                                                                                      var_list=tf.trainable_variables())\n",
    "        train_probabilities = tf.nn.softmax(logits)\n",
    "        train_predictions = tf.one_hot(tf.argmax(train_probabilities, 1), num_labels, dtype=tf.int32)\n",
    "        train_accuracy = slim.metrics.accuracy(train_predictions, tf_train_labels, 100.0)\n",
    "        valid_probabilities = tf.nn.softmax(network(tf_valid_dataset, True, False))\n",
    "        valid_predictions = tf.one_hot(tf.argmax(valid_probabilities, 1), num_labels, dtype=tf.int32)\n",
    "        valid_accuracy = slim.metrics.accuracy(valid_predictions, tf_valid_labels, 100.0)\n",
    "\n",
    "    init_op = tf.group(tf.initialize_all_variables(), \n",
    "                       tf.initialize_local_variables())\n",
    "    \n",
    "    # Add histograms for trainable variables.\n",
    "    for var in tf.trainable_variables():\n",
    "        tf.histogram_summary(var.op.name, var)\n",
    "        \n",
    "    # Add summaries for activations: NOT WORKING YET. TF ERROR.\n",
    "    #slim.summarize_activations()\n",
    "    \n",
    "    #Merge all summaries and write to a folder\n",
    "    merged_summs = tf.merge_all_summaries()\n",
    "    results_writer = tf.train.SummaryWriter('../results', graph)\n",
    "    \n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    #tracing for timeline\n",
    "    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "    run_metadata = tf.RunMetadata()    \n",
    "    \n",
    "print('computational graph created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 5001\n",
    "\n",
    "trace_file = open('../tracing/timeline.json', 'w')\n",
    "save_path = '../checkpoints/model.ckpt'\n",
    "\n",
    "best_loss = 99.0\n",
    "val_accu = 0.0\n",
    "best_val_accu = 0.0\n",
    "t = 0\n",
    "elapt = 0\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    ttotal = time.time()\n",
    "    init_op.run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        t = time.time()\n",
    "        offset = (step * batch_size) % (train_dataset.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "        _, l, trprob, traccu, summary = session.run(\n",
    "            [optimizer, loss, train_probabilities, train_accuracy, merged_summs], \n",
    "            feed_dict=feed_dict)\n",
    "        results_writer.add_summary(summary, step)\n",
    "        elapt = time.time()\n",
    "        if (step % 13 == 0):\n",
    "            best_loss = l if l < best_loss else best_loss\n",
    "            print('Minibatch total loss at step %d: %f' % (step, l), '| Best:', best_loss)\n",
    "            print('Minibatch accuracy:', traccu)\n",
    "            print('Predictions | Labels:\\n', np.concatenate((trprob[:2], batch_labels[:2]), axis=1))\n",
    "            print('Last iter time:', elapt-t)\n",
    "        if (step % 50 == 0):\n",
    "            val_accu = valid_accuracy.eval()\n",
    "            best_val_accu = val_accu if val_accu > best_val_accu else best_val_accu\n",
    "            print('###-> Validation accuracy:', val_accu, '| Best:', best_val_accu)\n",
    "    ettotal = time.time()\n",
    "    \n",
    "    print('Total time: %f hours' %((ettotal-ttotal)/3600.0))\n",
    "            \n",
    "    # Save tracing into disl\n",
    "    #trace = timeline.Timeline(step_stats=run_metadata.step_stats)\n",
    "    #trace_file.write(trace.generate_chrome_trace_format(show_memory=True))\n",
    "            \n",
    "    # Save the variables to disk.\n",
    "    saver.save(session, save_path)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "            \n",
    "    results_writer.flush()\n",
    "    results_writer.close()\n",
    "\n",
    "    print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
